{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T23:37:35.265406Z",
     "start_time": "2024-09-28T23:37:16.378775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    \"\"\"\n",
    "    Extract audio from a video file and save it as a WAV file.\n",
    "    \"\"\"\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(audio_path, codec='pcm_s16le')  # Save as PCM WAV\n",
    "    video.close()\n",
    "\n",
    "def classify_audio_chunks(audio_path, chunk_duration=10, confidence_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Classify 10-second chunks of the audio for emotions.\n",
    "    Returns a dictionary with start time as key and a tuple (predicted emotion, confidence) as value.\n",
    "    If confidence is below the threshold, the predicted emotion will be 'unknown'.\n",
    "    \"\"\"\n",
    "    # Load the model and feature extractor\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "    model = HubertForSequenceClassification.from_pretrained(\"xbgoose/hubert-speech-emotion-recognition-russian-dusha-finetuned\")\n",
    "    \n",
    "    # Define emotion mapping\n",
    "    num2emotion = {0: 'neutral', 1: 'angry', 2: 'positive', 3: 'sad', 4: 'other'}\n",
    "    \n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path, normalize=True)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sample_rate != 16000:  # Model expects 16kHz\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = transform(waveform)\n",
    "\n",
    "    # Ensure the audio input is mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono by averaging channels\n",
    "    \n",
    "    # Initialize the results dictionary\n",
    "    results = {}\n",
    "\n",
    "    # Calculate number of chunks\n",
    "    total_length = waveform.size(1) / 16000  # Total length in seconds\n",
    "    num_chunks = int(total_length // chunk_duration)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_time = i * chunk_duration\n",
    "        end_time = start_time + chunk_duration\n",
    "\n",
    "        # Extract the chunk\n",
    "        start_sample = int(start_time * 16000)\n",
    "        end_sample = int(end_time * 16000)\n",
    "        audio_chunk = waveform[:, start_sample:end_sample]\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        inputs = feature_extractor(\n",
    "            audio_chunk.squeeze(0),  # Remove the channel dimension\n",
    "            sampling_rate=feature_extractor.sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            max_length=16000 * chunk_duration,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs['input_values']).logits\n",
    "        \n",
    "        # Get the predicted class index and confidence scores\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "        confidence_scores = torch.softmax(logits, dim=-1)  # Get confidence scores\n",
    "        confidence = confidence_scores[0][predicted_class].item()  # Confidence for the predicted class\n",
    "        \n",
    "        # Map the predicted class to emotion, or set to 'unknown' if below threshold\n",
    "        if confidence >= confidence_threshold:\n",
    "            predicted_emotion = num2emotion[predicted_class]\n",
    "        else:\n",
    "            predicted_emotion = 'unknown'\n",
    "        \n",
    "        # Store the result with confidence\n",
    "        results[start_time] = (predicted_emotion, confidence)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example Usage\n",
    "# video_path = \"/home/pe51k/PycharmProjects/secret-repo/data/video/test/test_0.mp4\"\n",
    "video_path = \"/home/pe51k/PycharmProjects/secret-repo/data/video/test/Над расследованием по Северным потокам смеется вся Европа. Великий перепост [TubeRipper.com].mp4\"\n",
    "# video_path = \"/home/pe51k/PycharmProjects/secret-repo/data/video/test/ПОЛОСА ПРЕПЯТСТВИЙ ДЛЯ КОТА АБРИКОСА - Кусь-шоу Весёлые челленджи [TubeRipper.com].mp4\"\n",
    "audio_path = \"extracted_audio.wav\"\n",
    "\n",
    "# Step 1: Extract audio from video\n",
    "extract_audio_from_video(video_path, audio_path)\n",
    "\n",
    "# Step 2: Classify audio in chunks\n",
    "confidence_threshold = 0.7  # Set your desired confidence threshold\n",
    "classification_results = classify_audio_chunks(audio_path, confidence_threshold=confidence_threshold)\n",
    "\n",
    "# Output the classification results\n",
    "print(classification_results)"
   ],
   "id": "6cb7256a9146ffa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xbgoose/hubert-speech-emotion-recognition-russian-dusha-finetuned were not used when initializing HubertForSequenceClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at xbgoose/hubert-speech-emotion-recognition-russian-dusha-finetuned and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ('neutral', 0.898531436920166), 10: ('unknown', 0.5047914981842041), 20: ('unknown', 0.6240713000297546), 30: ('unknown', 0.4543779790401459), 40: ('angry', 0.758493185043335), 50: ('angry', 0.7123038172721863), 60: ('unknown', 0.6553771495819092), 70: ('unknown', 0.4523645341396332), 80: ('unknown', 0.616582989692688), 90: ('angry', 0.9587709307670593), 100: ('angry', 0.8447172045707703), 110: ('unknown', 0.6968732476234436), 120: ('unknown', 0.5530036687850952), 130: ('unknown', 0.5075026750564575), 140: ('angry', 0.8949615359306335), 150: ('unknown', 0.5432318449020386), 160: ('unknown', 0.5116928219795227), 170: ('neutral', 0.7228280305862427), 180: ('unknown', 0.6902819275856018), 190: ('unknown', 0.5846384167671204), 200: ('neutral', 0.9654733538627625), 210: ('neutral', 0.981601893901825), 220: ('neutral', 0.7543083429336548), 230: ('angry', 0.8148767948150635)}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T23:44:45.986919Z",
     "start_time": "2024-09-28T23:44:44.542871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def analyze_loudness(audio_path, chunk_duration=10, volume_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Analyze loudness of audio in chunks.\n",
    "    Returns a dictionary with start time as key and a tuple (mean volume, percent difference, loudness classification) as value.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(audio_path, normalize=True)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sample_rate != 16000:  # Model expects 16kHz\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = transform(waveform)\n",
    "\n",
    "    # Ensure the audio input is mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Convert to mono by averaging channels\n",
    "\n",
    "    # Calculate the mean volume of the entire audio file\n",
    "    audio_data = waveform.numpy()[0]  # Convert to numpy array\n",
    "    overall_mean_volume = np.mean(librosa.feature.rms(y=audio_data))\n",
    "\n",
    "    # Initialize the results dictionary\n",
    "    loudness_results = {}\n",
    "\n",
    "    # Calculate number of chunks\n",
    "    total_length = waveform.size(1) / 16000  # Total length in seconds\n",
    "    num_chunks = int(total_length // chunk_duration)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_time = i * chunk_duration\n",
    "        end_time = start_time + chunk_duration\n",
    "\n",
    "        # Extract the chunk\n",
    "        start_sample = int(start_time * 16000)\n",
    "        end_sample = int(end_time * 16000)\n",
    "        audio_chunk = waveform[:, start_sample:end_sample].numpy()[0]  # Convert to numpy array\n",
    "\n",
    "        # Calculate the mean volume for the chunk\n",
    "        chunk_mean_volume = np.mean(librosa.feature.rms(y=audio_chunk))\n",
    "        \n",
    "        # Calculate percent difference from overall mean volume\n",
    "        percent_diff = (chunk_mean_volume - overall_mean_volume) / overall_mean_volume * 100  # Convert to percentage\n",
    "\n",
    "        # Determine loudness classification\n",
    "        if percent_diff > volume_threshold * 100:\n",
    "            loudness_status = \"loud\"\n",
    "        elif percent_diff < -volume_threshold * 100:\n",
    "            loudness_status = \"quiet\"\n",
    "        else:\n",
    "            loudness_status = \"normal\"\n",
    "\n",
    "        # Store the result\n",
    "        loudness_results[start_time] = (chunk_mean_volume, percent_diff, loudness_status)\n",
    "\n",
    "    return loudness_results\n",
    "\n",
    "# Step 2: Analyze loudness in chunks\n",
    "volume_threshold = 0.1  # Set your desired volume threshold for loudness classification\n",
    "loudness_results = analyze_loudness(audio_path, chunk_duration=10, volume_threshold=volume_threshold)\n",
    "\n",
    "print(\"Loudness Analysis Results:\", loudness_results)"
   ],
   "id": "41d67e4a3a754153",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loudness Analysis Results: {0: (0.052737, -4.346724599599838, 'normal'), 10: (0.047385897, -14.052446186542511, 'quiet'), 20: (0.052177295, -5.3619083017110825, 'normal'), 30: (0.054364823, -1.3942159712314606, 'normal'), 40: (0.05919643, 7.369254529476166, 'normal'), 50: (0.05567366, 0.9797235950827599, 'normal'), 60: (0.055661913, 0.9584192186594009, 'normal'), 70: (0.056004174, 1.5792051330208778, 'normal'), 80: (0.05390702, -2.2245725616812706, 'normal'), 90: (0.05659374, 2.6485448703169823, 'normal'), 100: (0.05345071, -3.052212856709957, 'normal'), 110: (0.054948863, -0.3348967060446739, 'normal'), 120: (0.05337519, -3.1891945749521255, 'normal'), 130: (0.05855794, 6.211170554161072, 'normal'), 140: (0.056462802, 2.4110550060868263, 'normal'), 150: (0.056765426, 2.959948033094406, 'normal'), 160: (0.058398627, 5.922213569283485, 'normal'), 170: (0.04427266, -19.69916820526123, 'quiet'), 180: (0.05806135, 5.310468375682831, 'normal'), 190: (0.0502579, -8.84326919913292, 'normal'), 200: (0.06275149, 13.81734013557434, 'loud'), 210: (0.065787315, 19.323663413524628, 'loud'), 220: (0.062907085, 14.099560678005219, 'loud'), 230: (0.050390385, -8.602968603372574, 'normal')}\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
