{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c73a15-4d2e-4313-9a07-e92eabae2e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7f5348818c48db925771a620e106f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import av\n",
    "import torch\n",
    "from transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "model_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n",
    "\n",
    "model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ").to(0)\n",
    "\n",
    "processor = LlavaNextVideoProcessor.from_pretrained(model_id)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "            \n",
    "    frames = np.array([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "    # Now convert the whole array to a PyTorch tensor\n",
    "    return torch.tensor(frames)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0403b31f-0e00-4fba-b573-3edc04d692b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER: \n",
      "What's happening in the video? ASSISTANT: In the video, we see a dog walking on a snowy surface, possibly a trail or a path, with a body of water visible in the background. The dog appears to be walking towards the camera, and there are some trees and bushes in the background. The ground is covered in snow, and the dog's fur is visible, indicating it's a cold environment. The dog seems to be enjoying its walk and is captured mid-stride, possibly exploring its surr\n"
     ]
    }
   ],
   "source": [
    "# define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\", \"video\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's happening in the video?\"},\n",
    "            {\"type\": \"video\"},\n",
    "            ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "#video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n",
    "video_path = r\"C:\\Users\\Sergey\\Documents\\GitHub\\edok\\research\\test_video\\1.mp4\"\n",
    "container = av.open(video_path)\n",
    "\n",
    "# sample uniformly 8 frames from the video, can sample more for longer videos\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip = read_video_pyav(container, indices)\n",
    "inputs_video = processor(text=prompt, videos=clip, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs_video, max_new_tokens=100, do_sample=False)\n",
    "print(processor.decode(output[0][2:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7f464-67c1-4df9-9491-c16c4010b002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0fa7d-3881-4659-87ec-ae503d38973f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c428207-a8bf-4d32-973c-dc243f10b721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
