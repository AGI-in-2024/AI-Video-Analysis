# -*- coding: utf-8 -*-
"""scene_detect.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/118_R3Omutox8ypsTxRZoHOeDV9x7fcxl
"""

import av
import numpy as np
from scenedetect import detect, AdaptiveDetector, VideoManager, SceneManager
from scenedetect.detectors import ContentDetector, ThresholdDetector, HistogramDetector, HashDetector
from collections import Counter
import base64
import io
from PIL import Image

def analyze_complex_scenes(video_path):
    # Open the video file
    container = av.open(video_path)
    fps = eval(str(container.streams.video[0].average_rate))

    # Create VideoManager and SceneManager
    video_manager = VideoManager([video_path])
    scene_managers = [SceneManager() for _ in range(5)]

    # Add different detectors to each SceneManager
    scene_managers[0].add_detector(ContentDetector(threshold=30.0))
    scene_managers[1].add_detector(ThresholdDetector(threshold=12))
    scene_managers[2].add_detector(HistogramDetector(min_scene_len=15))
    scene_managers[3].add_detector(AdaptiveDetector(adaptive_threshold=3.0))
    scene_managers[4].add_detector(HashDetector())

    # Start VideoManager
    video_manager.start()

    # Detect scenes for each SceneManager
    all_scene_lists = []
    for scene_manager in scene_managers:
        scene_manager.detect_scenes(frame_source=video_manager)
        scenes = scene_manager.get_scene_list()
        all_scene_lists.append(scenes)
        video_manager.reset()

    # Merge and filter scene lists
    merged_scenes = merge_scene_lists(all_scene_lists, fps)

    # Extract frames for each scene
    scene_frames = extract_scene_frames(video_path, merged_scenes)

    return {
        "scenes": [
            {
                "start_frame": scene,
                "start_time": format_time(scene / fps),
                "preview_image": frame_to_base64(frame)
            }
            for scene, frame in zip(merged_scenes, scene_frames)
        ],
        "total_scenes": len(merged_scenes),
        "fps": fps,
        "duration": format_time(container.streams.video[0].duration / container.streams.video[0].time_base / fps)
    }

def merge_scene_lists(scene_lists, fps):
    fps_window = fps * 1.3
    all_scene_list = [scene_lists[0][0][0].get_frames()]
    all_scene_list.extend(scene[1].get_frames() for scene_list in scene_lists for scene in scene_list)

    scene_counter = Counter(all_scene_list)
    scene_counter_key = sorted(scene_counter.keys())

    combined_scenes = []
    current_scene = scene_counter_key[0]

    for next_scene in scene_counter_key[1:]:
        if next_scene - current_scene <= fps_window:
            if scene_counter[next_scene] > scene_counter[current_scene]:
                scene_counter[next_scene] += scene_counter[current_scene] / 2
                del scene_counter[current_scene]
                current_scene = next_scene
            else:
                scene_counter[current_scene] += scene_counter[next_scene] / 2
                del scene_counter[next_scene]
        else:
            current_scene = next_scene

    filtered_scene_counter = {k: v for k, v in scene_counter.items() if v > 2}
    return sorted(filtered_scene_counter.keys())

def extract_scene_frames(video_path, scene_list):
    frames = []
    with av.open(video_path) as container:
        stream = container.streams.video[0]
        for frame_idx, frame in enumerate(container.decode(video=0)):
            if frame_idx in scene_list:
                frames.append(frame.to_image())
            if frame_idx > max(scene_list):
                break
    return frames

def frame_to_base64(frame):
    buffered = io.BytesIO()
    frame.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

def format_time(seconds):
    minutes, seconds = divmod(int(seconds), 60)
    hours, minutes = divmod(minutes, 60)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"